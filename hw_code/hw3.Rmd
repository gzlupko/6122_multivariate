---
title: "hw3"
author: "Gian Zlupko"
date: "2023-02-20"
output: html_document
---

### HUDM 6122 Multivariate  
#### HW3 


Libraries Used
```{r, message = F, warning = F}
library(tidyverse)
library(HSAUR) # hepthalon data 
library(HSAUR2) # air pollution data 

```




#### Ex. 3.1 *Construct the scatterplot of the heptathlon data showing the contours of the estimated bivariate density function on each panel. Is this graphic more useful than the unenhanced scatterplot matrix?*


```{r, warning = F, message = F}
data("heptathlon") 
# inspect data 
head(heptathlon) 
library(GGally) 

dat = heptathlon[, -8]
ggpairs(dat, diag = list(continuous = wrap("densityDiag", alpha = 0.5)))  

```



```{r}
plot(dat, pch = ".", cex = 1.5)
```




```{r}

# scatter plot without 'score'; col. no 8 
plot(heptathlon[, -8])
library(QRM) 
panel.hist <- function(x, ...) {
     usr <- par("usr"); on.exit(par(usr))
     par(usr = c(usr[1:2], 0, 1.5) )
     BiDensPlot(dat, xpts = c(-2, 2), ypts = c(-2, 2), npts = 50,
           type = c("persp", "contour"), ...)

}

panel.hist

USairpollution$negtemp <- USairpollution$temp * (-1)

USairpollution$temp <- NULL

pairs(dat[,-1], diag.panel = panel.hist, 
       pch = ".", cex = 1.5)
```


note: use code from pg.51 and instead of using hist() use density on the panels like they do on pg. 87

```{r}
data("USairpollution", package = "HSAUR2")
pollution <- USairpollution


density(pollution$SO2, pollution$wind) 



pollution 
panel.hist <- function(x, ...) {
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5)) 
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col="grey", ...)}

pollution$negtemp <- pollution$temp * (-1)
pollution$temp <- NULL
pairs(pollution[,-1], diag.panel = panel.hist, pch = ".", cex = 1.5)



```






#### Ex. 3.2 *Construct a diagram that shows the SO2 variable in the air pollution data plotted against each of the six explanatory variables, and in each of the scatterplots show the fitted linear regression and a fitted locally weighted regression. Does this diagram help in deciding on the most appropriate model for determining the variables most predictive of sulphur dioxide levels?*

```{r}

data("USairpollution", package = "HSAUR2")
pollution <- USairpollution
head(pollution) 

# linear reg plotted in red; locally fitted reg in blue
pairs(pollution, 
      panel = function(x, y) { 
        
        points(x, y)
        abline(lm(y ~ x), col = "red")
        lines(stats::lowess(x, y))
        })

```

The scatterplot with the linear and locally fit regression models for each pairs shows that there are predictor variables that appear to be moderately correlated with SO2, such as 'manu' and 'temp'. However, there are discrepancies between the linear regression line and the locally fitted line in most of the bivariate relationships shown with SO2, including 'manu' and 'temp', suggesting that the relationship between these variables and SO2 may be more complex than a simple linear fitted line may indicate.



#### Ex. 3.3 *Find the principal components of the following correlation matrix given by MacDonnell (1902) from measurements of seven physical characteristics in each of 3000 convicted criminals:*
```{r}

# read in corr matrix 
crim_cor <- matrix(c(1.000, 0.402, 1.000, 0.396, 0.618, 1.000,0.301, 0.150, 0.321, 1.000, 
0.305, 0.135, 0.289, 0.846, 1.000, 0.339, 0.206, 0.363, 0.759, 0.797, 1.000, 0.340, 0.183, 0.345, 0.661, 0.800, 0.736, 1.000), nrow = 7 , ncol = 7)
crim_cor 

# use PCA on the corr matrix 

crim_pca <- princomp(crim_cor, cor = TRUE)
summary(crim_pca) 

```


#### *How would you interpret the derived components?*

Results from PCA indicated that the first two principal components accounted for 70% of the variance in the original seven varaibles. The third principal component accounted for an additional 24% of the variation in the original data and, collectively, three components represented nearly 95% of the overall variance in the original seven variables. These findings indicate that two or three components (depending on how much of the original variance a researcher would want to capture) would effectively summarize the original variance in the data set. Moreover, we can look back at the correlation matrix and identify potential combinations of the original variables that are likely underlying the components. For example, head length and head breadth are moderately correlated. Separately, forearm length, foot length and height are all strongly correlated.



#### Ex. 3.4 *Not all canonical correlations may be statistically significant. An approximate test proposed by Bartlett (1947) can be used to determine how many significant relationships exist. The test statistic for testing that at least one canonical correlation is significant is Φ20 = − 􏰃n − 12 (q1 + q2 + 1)􏰄 􏰐si=1 log(1 − λi), where the λi are the eigenvalues of E1 and E2. Under the null hypothesis that all correlations are zero, Φ20 has a chi-square distribution with q1 ×q2 degrees of freedom. Write R code to apply this test to the headsize data (Table 3.1) and the depression data (Table 3.3).*



```{r}

```



depression data 
```{r}

# head size data
data("frets", package = "boot")
headsize <- frets


depr <- c(
  0.212,
  0.124,  0.098,
 -0.164,  0.308,  0.044,
 -0.101, -0.207, -0.106, -0.208,
 -0.158, -0.183, -0.180, -0.192, 0.492)

LAdepr <- diag(6) / 2

LAdepr[upper.tri(LAdepr)] <- depr

LAdepr <- LAdepr + t(LAdepr)

rownames(LAdepr) <- colnames(LAdepr) <- c("CESD", "Health", "Gender", "Age", "Edu", "Income")

x <- LAdepr

LAdepr <- as.data.frame(LAdepr)

#headsize
headsize.std <- sweep(headsize, 2, apply(headsize, 2, sd), FUN = "/")
R <- cor(headsize.std)
r11 <- R[1:2, 1:2]
r22 <- R[-(1:2), -(1:2)]
r12 <- R[1:2, -(1:2)]
r21 <- R[-(1:2), 1:2]
(E1 <- solve(r11) %% r12 %% solve(r22) %*%r21)
(E2 <- solve(r22) %% r21 %% solve(r11) %*%r12)
e1 <- eigen(E1)
e1
e2 <-eigen(E2)
e2
headsize.std <- t(headsize.std) %*% as.matrix(headsize.std)
girth1 <- headsize.std[,1:2] %*% e1$vectors[,1]
girth2 <- headsize.std[,3:4] %*% e2$vectors[,1]

shape1 <- headsize.std[,1:2] %*% e1$vectors[,2]
shape2 <- headsize.std[,3:4] %*% e2$vectors[,2]
g <- cor(girth1, girth2)
s <- cor(shape1, shape2)


# Compute the eigenvalues of E1 and E2
lambda <- e1$values 
q1 <- ncol(E1)
q2 <- ncol(E2) 

# Compute the test statistic
n <- nrow(E1)
Phi2 <- -(n-(df+1)/2) * sum(log(1 - lambda))
df <- q1 * q2
p_value <- pchisq(Phi2, df, lower.tail = FALSE)
```






previous attempt 

```{r}

# load headsize df 
data("frets", package = "boot")
headsize <- frets

# inspect headsize df 
head(headsize) 


# load depression df 

depr <- c(
  0.212,
  0.124,  0.098,
 -0.164,  0.308,  0.044,
 -0.101, -0.207, -0.106, -0.208,
 -0.158, -0.183, -0.180, -0.192, 0.492)

LAdepr <- diag(6) / 2

LAdepr[upper.tri(LAdepr)] <- depr

LAdepr <- LAdepr + t(LAdepr)

rownames(LAdepr) <- colnames(LAdepr) <- c("CESD", "Health", "Gender", "Age", "Edu", "Income")

x <- LAdepr

la_dep <- as.data.frame(LAdepr)

# inspect depression df 

head(la_dep)


```



```{r}
headsize.std <- sweep(headsize, 2, apply(headsize, 2, sd), FUN = "/")
R <- cor(headsize.std)
r11 <- R[1:2, 1:2]
r22 <- R[-(1:2), -(1:2)]
r12 <- R[1:2, -(1:2)]
r21 <- R[-(1:2), 1:2]
(E1 <- solve(r11) %% r12 %% solve(r22) %*%r21)
(E2 <- solve(r22) %% r21 %% solve(r11) %*%r12)
e1 <- eigen(E1)
e1
e2 <-eigen(E2)
e2
headsize.std <- t(headsize.std) %*% as.matrix(headsize.std)
girth1 <- headsize.std[,1:2] %*% e1$vectors[,1]
girth2 <- headsize.std[,3:4] %*% e2$vectors[,1]

shape1 <- headsize.std[,1:2] %*% e1$vectors[,2]
shape2 <- headsize.std[,3:4] %*% e2$vectors[,2]
g <- cor(girth1, girth2)
s <- cor(shape1, shape2)


# Compute the eigenvalues of E1 and E2
lambda <- e1$values 
q1 <- ncol(E1)
q2 <- ncol(E2) 

# Compute the test statistic
n <- nrow(E1)
Phi2 <- -(n-(df+1)/2) * sum(log(1 - lambda))
df <- q1 * q2
p_value <- pchisq(Phi2, df, lower.tail = FALSE
```



```{r}
require(CCA) 
```




Depression data 

```{r}
#load depression data
depr <- c(
 0.212,
 0.124,  0.098,
-0.164,  0.308,  0.044,
-0.101, -0.207, -0.106, -0.208,
-0.158, -0.183, -0.180, -0.192, 0.492)
LAdepr <- diag(6) / 2
LAdepr[upper.tri(LAdepr)] <- depr
LAdepr <- LAdepr + t(LAdepr)
rownames(LAdepr) <- colnames(LAdepr) <- c("CESD", "Health", "Gender", "Age", "Edu", "Income")
x <- LAdepr

x
LAdepr <- as.data.frame(LAdepr)


```



#### Ex. 3.5 *Repeat the regression analysis for the air pollution data described in the text after removing whatever cities you think should be regarded as outliers. For the results given in the text and the results from the outliers-removed data, produce scatterplots of sulphur dioxide concentra- tion against each of the principal component scores. Interpret your results.*

```{r}

```































