---
title: "hw6"
author: "Gian Zlupko"
date: "2023-03-29"
output: html_document
---


*Ex. 6.1 Apply k-means to the crime rate data after standardizing each variable by its standard deviation. Compare the results with those given in the text found by standardizing by a variableâ€™s range.*

First I create the crime data set.

```{r}
crime <- structure(c(2, 2.2, 2, 3.6, 3.5, 4.6, 10.7, 5.2, 5.5, 5.5, 6, 8.9, 11.3, 3.1, 2.5, 1.8, 9.2, 1, 4, 3.1, 4.4, 4.9, 9, 31, 7.1,
5.9, 8.1, 8.6, 11.2, 11.7, 6.7, 10.4, 10.1, 11.2, 8.1, 12.8,
8.1, 13.5, 2.9, 3.2, 5.3, 7, 11.5, 9.3, 3.2, 12.6, 5, 6.6, 11.3,
8.6, 4.8, 14.8, 21.5, 21.8, 29.7, 21.4, 23.8, 30.5, 33.2, 25.1, 38.6, 25.9, 32.4, 67.4, 20.1, 31.8, 12.5, 29.2, 11.6, 17.7, 24.6, 32.9, 56.9, 43.6, 52.4, 26.5, 18.9, 26.4, 41.3, 43.9, 52.7, 23.1, 47, 28.4, 25.8, 28.9, 40.1, 36.4, 51.6, 17.3, 20, 21.9, 42.3, 46.9, 43, 25.3, 64.9, 53.4, 51.1, 44.9, 72.7, 31, 28, 24, 22, 193, 119, 192, 514, 269, 152, 142, 90, 325, 301, 73, 102, 42, 170, 7, 16, 51, 80, 124, 304, 754, 106, 41, 88, 99, 214, 367, 83, 208, 112, 65, 80, 224, 107, 240, 20, 21, 22, 145, 130, 169, 59, 287, 135, 206, 343, 88, 106, 102, 92, 103, 331, 192, 205, 431, 265, 176, 235, 186, 434, 424, 162, 148, 179, 370, 32, 87,184, 252, 241, 476, 668, 167, 99, 354, 525, 319, 605, 222, 274, 408, 172, 278, 482, 285, 354, 118, 178, 243, 329, 538, 437, 180,354, 244, 286, 521, 401, 103, 803, 755, 949, 1071, 1294, 1198, 1221, 1071, 735, 988, 887, 1180, 1509, 783, 1004, 956, 1136, 385, 554, 748, 1188, 1042, 1296, 1728, 813, 625, 1225, 1340, 1453, 2221, 824, 1325, 1159, 1076, 1030, 1461, 1787, 2049, 783, 1003, 817, 1792, 1845, 1908, 915, 1604, 1861, 1967, 1696, 1162, 1339, 2347, 2208, 2697, 2189, 2568, 2758, 2924, 2822, 1654, 2574,2333, 2938, 3378, 2802, 2785, 2801, 2500, 2049, 1939, 2677, 3008, 3090, 2978, 4131, 2522, 1358, 2423, 2846, 2984, 4373, 1740, 2126, 2304, 1845, 2305, 3417, 3142, 3987, 3314, 2800, 3078, 4231, 3712, 4337, 4074, 3489, 4267, 4163, 3384, 3910, 3759, 164, 228, 181, 906, 705, 447, 637, 776, 354, 376, 328, 628, 800, 254, 288, 158, 439, 120, 99, 168, 258, 272, 545, 975, 219, 169, 208, 277, 430, 598, 193, 544, 267, 150, 195, 442, 649, 714, 215, 181, 169, 486, 343, 419, 223, 478, 315, 402, 762, 604, 328), .Dim = c(51L, 7L), .Dimnames = list(c("ME", "NH", "VT", "MA", "RI", "CT", "NY", "NJ", "PA", "OH", "IN", "IL", "MI", "WI", "MN", "IA", "MO", "ND",
"SD", "NE", "KS", "DE", "MD", "DC", "VA", "WV", "NC", "SC", "GA",
"FL", "KY", "TN", "AL", "MS", "AR", "LA", "OK", "TX", "MT", "ID",
"WY", "CO", "NM", "AZ", "UT", "NV", "WA", "OR", "CA", "AK", "HI"), c("Murder", "Rape", "Robbery", "Assault", "Burglary", "Theft","Vehicle")))

crime <- as.data.frame(crime)

# inspect the data set
head(crime)
```

Next I scale the data by dividing each variable by its standard deviation. 

```{r}
crime_scaled  <- as.data.frame(scale(crime))

# check that data was scaled appropriately 
for (i in 1:ncol(crime_scaled)) {
  print(sd(crime_scaled[,i]))
}
```


Next I apply k-means to the standardized crime data. I examine within-group sum of squares (WGSS) as a method for determining how my `k` clusters to fit. Following the authors' direction, I examine two to six clusters and use the elbow method to evaluate which number of clusters sufficiently minimizes WGSS. 


```{r}
n <- nrow(crime_scaled)
wss <- rep(0,6)
wss[1] <- (n - 1) * sum(sapply(crime_scaled, var))
for (i in 2:6)
  wss[i] <- sum(kmeans(crime_scaled, 
                       centers = i)$withinss)
plot(1:6, wss, type = "b", xlab ="Number of groups", 
     ylab = "Within groups sum of squares")
```

Results indicate that two clusters is sufficiently minimizes WGSS, suggesting that the data contain two clusters. In contrast, using a separate data standardization technique, the authors found that the data contain three clusters. Differences in the values of `k` should be expected when separate data transformations are applied to the raw data before using k-means. 



*Ex. 6.2 Calculate the first five principal components scores for the Romano- British pottery data, and then construct the scatterplot matrix of the scores, displaying the contours of the estimated bivariate density for each panel of the plot and a box plot of each score in the appropriate place on the diagonal. Label the points in the scatterplot matrix with their kiln numbers.*

First I load the pottery data and inspect it. I also remove the kiln variable as it is a categorical variable and therefore not able to be used with PCA. 
```{r, message = F}
library(tidyverse)
library(HSAUR2)
data(pottery)

# remove kiln 
pottery_sub <- pottery %>% 
  select(-kiln)

# inspect data varainces
for (i in 1:ncol(pottery_sub)) { 
  print(var(pottery_sub[, i]))
  }
```
By inspecting the variances we see that the scales are very different for each variable. Therefore, before using PCA, I standardize the data.

```{r}
pottery_scaled <- as.data.frame(scale(pottery_sub)) 

# confirm that data was scaled appropriately
for (i in 1:ncol(pottery_scaled)) { 
  print(var(pottery_scaled[, i]))
  }
```

```{r}

```




*Ex. 6.3 Return to the air pollution data given in Chapter 1 and use finite mixtures to cluster the data on the basis of the six climate and ecology variables (i.e., excluding the sulphur dioxide concentration). Investigate how sulphur dioxide concentration varies in the clusters you find both graphically and by formal significance testing.*


```{r}

```






```{r}
library(MVA)
demo("Ch-CA")
```



